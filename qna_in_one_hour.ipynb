{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdIV99zuPl3a9yP65nTqM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackycsl/bitcoin_ticker/blob/main/qna_in_one_hour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QuickStart"
      ],
      "metadata": {
        "id": "ZX3zZIeylpgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq langchain"
      ],
      "metadata": {
        "id": "WzdHfU7_cr9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq openai"
      ],
      "metadata": {
        "id": "uJqdKeh5c2UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "6lBfrUiblQ56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cDHwlHq0bglZ"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm  = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"hello, this is a test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "iRC4RD62e1FD",
        "outputId": "88b7a29d-4089-4ce1-8a2e-065a61fa81b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nHello! It's nice to meet you. What kind of test are you running?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "U1CTH3U7c294"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(llm(\"What is zero-shot chain-of-thought prompting?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "2REIajB9lVaB",
        "outputId": "23717168-c230-4b74-e142-5ab0ce7bbae1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "?\n\nZero-shot chain-of-thought prompting is a technique used to help people with cognitive impairments to generate ideas and thoughts. It involves prompting the person with a series of questions that are related to the topic at hand. The questions are designed to help the person to think of related ideas and to build a chain of thought. This technique can be used to help people with memory problems, language impairments, and other cognitive impairments to generate ideas and thoughts."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's bring in some context.\n"
      ],
      "metadata": {
        "id": "_QTgPXN2lzxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDsG9MM_l2Qr",
        "outputId": "8fc612c7-5fdf-4552-f900-ffe434643880"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "paper = next(arxiv.Search(id_list=[\"2205.11916\"]).results())\n",
        "\n",
        "Markdown(paper.summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "7enk8AW3l4-o",
        "outputId": "c27627d3-20fe-450b-afb7-57da690bac9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars."
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(f\"\"\"Here's a summary of a paper:\n",
        "{paper.summary}\n",
        "\n",
        "Based on that summary, what is zero-shot chain-of-thought prompting?\"\"\"\n",
        ")\n",
        "\n",
        "Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "mYny3Y0lmOPH",
        "outputId": "aeb403ae-82bf-43e9-a365-c4a6b6396763"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nZero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples, using a single prompt template, without any hand-crafted few-shot examples. It has been shown to significantly outperform zero-shot language model performances on diverse benchmark reasoning tasks, including arithmetics, symbolic reasoning, and other logical reasoning tasks."
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the paper searchable"
      ],
      "metadata": {
        "id": "iiUewbmhmpCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pKOeSK3mogJ",
        "outputId": "c4268a5e-ad8a-4ff9-a0f6-6da6509b1b34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.8/248.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_path = paper.download_pdf()"
      ],
      "metadata": {
        "id": "QGd0SWD-mU-N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(paper_path)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "UVPwQvgpmzu0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BbAfQrbnBBA",
        "outputId": "373fd58e-13b1-414d-c893-448b9d8f36ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])"
      ],
      "metadata": {
        "id": "mNzZEUpqnDBZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(f\"\"\"Here's the first two pages of a paper:\n",
        "{content}\n",
        "\n",
        "Based on that content, what is zero-shot chain-of-thought prompting?\"\"\")\n",
        "\n",
        "Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "cSJFn3ZDnPZa",
        "outputId": "33079763-8094-48d6-e55a-19e3bcb74c06"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nZero-shot chain-of-thought prompting is a technique for eliciting complex multi-step reasoning through step-by-step answer examples without the need for any hand-crafted few-shot examples. It involves adding a prompt, such as \"Let's think step by step,\" before each answer to facilitate step-by-step thinking. This technique has been shown to significantly outperform zero-shot language model performances on diverse benchmark reasoning tasks, including arithmetics, symbolic reasoning, and other logical reasoning tasks."
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the relevant content using embedding search"
      ],
      "metadata": {
        "id": "Nfy1R5R0nfXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq faiss-cpu tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPyovOeOoGK8",
        "outputId": "46f4aa36-b14b-442a-a135-3951d6113ea5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "xOjJGpM3oz6L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "jVm3lj8YpRvW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(\"what is zeo-shot chain-of-thought prompting?\")"
      ],
      "metadata": {
        "id": "mtWaOKxhpt20"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "HzdCGZgQp7eU",
        "outputId": "38ab3374-5320-4a9d-d011-d66087b6bea6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard\nfew-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a\ngiven for tackling such difﬁcult tasks, and the zero-shot baseline performances were not even reported\nin the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022]\nasFew-shot-CoT in this work.\n3 Zero-shot Chain of Thought\nWe propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning.\nIt differs from the original chain of thought prompting [Wei et al., 2022] as it does not require\nstep-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al.,\n2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks\nwith a single template. The core idea of our method is simple, as described in Figure 1: add Let’s"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use an existing chain"
      ],
      "metadata": {
        "id": "iXhN0pCPqA6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ],
      "metadata": {
        "id": "hnZ5SFwJqD09"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
        "query = \"What is zero-shot chain-of-thought prompting\"\n",
        "sources = db.similarity_search(query)\n",
        "results = chain({\"input_documents\": sources, \"question\": query}, return_only_outputs=False)"
      ],
      "metadata": {
        "id": "4cRbkDFxrEj5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(results[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "eDWRGA1ZrJU1",
        "outputId": "4bc1cdd6-ccc5-42b4-b9a2-27b9b41eee7e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Zero-shot chain-of-thought prompting is a template-based prompting for chain of thought reasoning that does not require step-by-step few-shot examples and is task-agnostic, eliciting multi-hop reasoning across a wide range of tasks with a single template.\nSOURCES: ./2205.11916v4.Large_Language_Models_are_Zero_Shot_Reasoners.pdf"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3UFeXTZsadG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}